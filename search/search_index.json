{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to HRI Teleoperation Tutorial If you wish to develop cool algorithm or experiments using our teleoperation platform, you are in the right place! Code Editor You will need vscode , so download it and install it before diving in! The tutorial section will guide you to the installation of all of the software components you will need!","title":"Home"},{"location":"#welcome-to-hri-teleoperation-tutorial","text":"If you wish to develop cool algorithm or experiments using our teleoperation platform, you are in the right place!","title":"Welcome to HRI Teleoperation Tutorial"},{"location":"#code-editor","text":"You will need vscode , so download it and install it before diving in! The tutorial section will guide you to the installation of all of the software components you will need!","title":"Code Editor"},{"location":"tutorial/","text":"Tutorials (Beginner) At Artificial Hands Area, we use complex devices to pursue our research on Human-Robot Interaction, particularly teleoperation platforms. Our platform is composed of the following: Robotic arms (Universal Robots) Anthropomorphic grippers (Prensilia Mia Hand) Then on the operator side, we use a collection of motion capture (mocap) devices: Wearable IMUs suit (Perception Neuron) Exogloves (Senseglove) Reflective markers mocap (Optitrack) In the end, mocap systems allow us to remap human motions on the robotic platform. Docker As you can imagine dealing with all of these components at once is non-trivial and requires a lot of programming. To make our life easier, we decided to develop a comprehensive software architecture, so that you only need to plugin/wear the devices and you're ready to go! Spoiler: We are still developing, thus it's not that easy yet, but it will be! Have you ever heard the citation \"on my computer it works\"? It's very common that you will say those words if you ever share your code with your colleagues. Then to prevent them from replying \"On mine it doesn't\", we decided in our lab not only to share the code but rather the whole pc. How? Virtually. Using Docker Containers. To put it simply, a container is a virtual machine which contains the proper operating system configuration that supports your code. When you're ready to share your code, you send the whole container (well actually a copy of it which is called an Image). Your colleague will just turn on the container to have your code integrated into its own system. And so on. Linux In Linux, you can follow the install using the apt repository guide. Note: apt is a repository of all packages that you can install on Linux. As you will notice sudo will require your password, which means that docker requires you to log in as an administrator to run any command, like starting the hello-world container. Asking for your password is fine if you need to install a library once using apt , but it's annoying if you need it for any docker command. So, let's say to docker that you can work with containers also as a non-root (i.e., non-administrator) user. For this, we create a new group of users sudo groupadd docker and we say that you (the user) are part of that group sudo usermod -aG docker $USER Then we say to our operating system that there is a new group of users newgrp docker and now starting a container is way simpler docker run hello-world The point is that docker is now seeing that you (user) are part of a group named ... docker , so he's happy and it will let you work without asking for your password! Finally, you can set Docker Engine to automatically start on boot whenever you turn on your pc. Windows In Windows things may look way easier, but remember, Docker will consume a lot of RAM (not really a lot but at least 8 GB), and we only tested with Windows 11 (you can try with 10, and let us know). Actually, the Docker Engine will run on the Windows Subsystem for Linux (WSL). So you need to run the install WSL command first. Then simply download and install Docker Desktop (it will include the Engine!). Open the application and try from the Windows PowerShell docker run hello-world ROS As we have many devices, our platform requires a lot of specialized code (actually executables) to communicate with each of them, for instance to: Control the robots (make things move!) Read data from mocap devices (understand how they should move!) Moreover, how each of these specialized executables can communicate? For instance how the program that reads the posture of the human operator can send the information to the robot, to set its joint positions? Lucky us, we can use the Robot Operating System (ROS). ROS it's not actually an operating system, it's a framework where specialized programs are called nodes . The beauty of nodes is that they can exchange data very easily using ROS communication channels, which are called topics . Moreover, we do not need to write specialized programs for each device we buy. Since ROS it's a standard in robotics research, vendors typically provide ROS driver packages that you can install with apt . So in this way, the communication with the devices is solved. Our job is to develop our own ROS packages, to connect them. Make sure you have the prerequisites before starting with the tutorials on usage and programming of ROS applications. Also, use a proper container to work with the tutorials docker run --volume /tmp/.X11-unix:/tmp/.X11-unix --env DISPLAY=$DISPLAY -dit --name jazzy osrf/ros:jazzy-desktop Windows: Make sure you execute the docker run command from a WSL terminal, which you can access by running the wsl command in a Windows PowerShell. If you want to open a terminal to execute commands in the container docker exec -it jazzy bash Note: Within the container, you will always be the administrator, so you DON'T need sudo authentication. Note: See in the prerequisites how to use vscode to work within the container. Whenever you want to take a break or resume your work, you can simply stop/start the container docker stop jazzy #to stop it docker start jazzy #to restart it Prerequisites C++ As you can imagine you need to write some code to develop ROS packages, which you can do using: C++ Python C++ it's extremely performant, since it's a compiled language, the code you write is turned into binary instructions before the execution of the program, thanks to a tool which is called ... compiler . However, this comes at the cost of complicated syntax, which means the C++ codes are not easy to understand if you don't have the basics. On the other side, Python codes are way easier to read and write, but they are poorly performant. Indeed the code is not pre-compiled, but's is interpreted during the execution and sent to the machine again as a set of binary instructions. Things are way slower. We strongly suggest to learn the C++ basics for three reasons: If you understand C++, you will reuse a lot of that knowledge for Python. The opposite doesn't work. At a certain point you will find a ROS package (not yours) that you need to adjust to your will, for your specific application. What if they are written in C++? What if for your application you realize that Python code is not performant enough? Bonus: To get confident with docker in the meanwhile, we suggest you create a Linux container where you'll work to learn the basics (thus skip the Introduction part of the video tutorial) docker run -d -t --name ubuntu ubuntu #start a basic Linux container docker exec -it ubuntu bash -c 'apt update && apt install g++ -y' #install the C++ compiler, you will need it! Note: See how you can attach vscode to the container to start developing! CMake As you will learn from the video tutorial, you need to manually call the C++ compiler, which is the g++ executable, to turn your human-readable code into binary instructions for the machine. Imagine that your project includes many programs to compile. Do you need to call g++ manually for each file? No, of course, you can use CMake! CMake is a cross-platform (Windows/Linux) tool that makes the generation of executables easier. No wonder that is the tool you will need to use to compile the code you'll write to form a new ROS package. For this purpose, you will learn to use this tool in the ROS tutorials, but even here a minimum preliminary understanding of CMake is required. Completing Step 1 and Step 2 of the official tutorial will be enough!","title":"Tutorials (Beginner)"},{"location":"tutorial/#tutorials-beginner","text":"At Artificial Hands Area, we use complex devices to pursue our research on Human-Robot Interaction, particularly teleoperation platforms. Our platform is composed of the following: Robotic arms (Universal Robots) Anthropomorphic grippers (Prensilia Mia Hand) Then on the operator side, we use a collection of motion capture (mocap) devices: Wearable IMUs suit (Perception Neuron) Exogloves (Senseglove) Reflective markers mocap (Optitrack) In the end, mocap systems allow us to remap human motions on the robotic platform.","title":"Tutorials (Beginner)"},{"location":"tutorial/#docker","text":"As you can imagine dealing with all of these components at once is non-trivial and requires a lot of programming. To make our life easier, we decided to develop a comprehensive software architecture, so that you only need to plugin/wear the devices and you're ready to go! Spoiler: We are still developing, thus it's not that easy yet, but it will be! Have you ever heard the citation \"on my computer it works\"? It's very common that you will say those words if you ever share your code with your colleagues. Then to prevent them from replying \"On mine it doesn't\", we decided in our lab not only to share the code but rather the whole pc. How? Virtually. Using Docker Containers. To put it simply, a container is a virtual machine which contains the proper operating system configuration that supports your code. When you're ready to share your code, you send the whole container (well actually a copy of it which is called an Image). Your colleague will just turn on the container to have your code integrated into its own system. And so on.","title":"Docker"},{"location":"tutorial/#linux","text":"In Linux, you can follow the install using the apt repository guide. Note: apt is a repository of all packages that you can install on Linux. As you will notice sudo will require your password, which means that docker requires you to log in as an administrator to run any command, like starting the hello-world container. Asking for your password is fine if you need to install a library once using apt , but it's annoying if you need it for any docker command. So, let's say to docker that you can work with containers also as a non-root (i.e., non-administrator) user. For this, we create a new group of users sudo groupadd docker and we say that you (the user) are part of that group sudo usermod -aG docker $USER Then we say to our operating system that there is a new group of users newgrp docker and now starting a container is way simpler docker run hello-world The point is that docker is now seeing that you (user) are part of a group named ... docker , so he's happy and it will let you work without asking for your password! Finally, you can set Docker Engine to automatically start on boot whenever you turn on your pc.","title":"Linux"},{"location":"tutorial/#windows","text":"In Windows things may look way easier, but remember, Docker will consume a lot of RAM (not really a lot but at least 8 GB), and we only tested with Windows 11 (you can try with 10, and let us know). Actually, the Docker Engine will run on the Windows Subsystem for Linux (WSL). So you need to run the install WSL command first. Then simply download and install Docker Desktop (it will include the Engine!). Open the application and try from the Windows PowerShell docker run hello-world","title":"Windows"},{"location":"tutorial/#ros","text":"As we have many devices, our platform requires a lot of specialized code (actually executables) to communicate with each of them, for instance to: Control the robots (make things move!) Read data from mocap devices (understand how they should move!) Moreover, how each of these specialized executables can communicate? For instance how the program that reads the posture of the human operator can send the information to the robot, to set its joint positions? Lucky us, we can use the Robot Operating System (ROS). ROS it's not actually an operating system, it's a framework where specialized programs are called nodes . The beauty of nodes is that they can exchange data very easily using ROS communication channels, which are called topics . Moreover, we do not need to write specialized programs for each device we buy. Since ROS it's a standard in robotics research, vendors typically provide ROS driver packages that you can install with apt . So in this way, the communication with the devices is solved. Our job is to develop our own ROS packages, to connect them. Make sure you have the prerequisites before starting with the tutorials on usage and programming of ROS applications. Also, use a proper container to work with the tutorials docker run --volume /tmp/.X11-unix:/tmp/.X11-unix --env DISPLAY=$DISPLAY -dit --name jazzy osrf/ros:jazzy-desktop Windows: Make sure you execute the docker run command from a WSL terminal, which you can access by running the wsl command in a Windows PowerShell. If you want to open a terminal to execute commands in the container docker exec -it jazzy bash Note: Within the container, you will always be the administrator, so you DON'T need sudo authentication. Note: See in the prerequisites how to use vscode to work within the container. Whenever you want to take a break or resume your work, you can simply stop/start the container docker stop jazzy #to stop it docker start jazzy #to restart it","title":"ROS"},{"location":"tutorial/#prerequisites","text":"","title":"Prerequisites"},{"location":"tutorial/#c","text":"As you can imagine you need to write some code to develop ROS packages, which you can do using: C++ Python C++ it's extremely performant, since it's a compiled language, the code you write is turned into binary instructions before the execution of the program, thanks to a tool which is called ... compiler . However, this comes at the cost of complicated syntax, which means the C++ codes are not easy to understand if you don't have the basics. On the other side, Python codes are way easier to read and write, but they are poorly performant. Indeed the code is not pre-compiled, but's is interpreted during the execution and sent to the machine again as a set of binary instructions. Things are way slower. We strongly suggest to learn the C++ basics for three reasons: If you understand C++, you will reuse a lot of that knowledge for Python. The opposite doesn't work. At a certain point you will find a ROS package (not yours) that you need to adjust to your will, for your specific application. What if they are written in C++? What if for your application you realize that Python code is not performant enough? Bonus: To get confident with docker in the meanwhile, we suggest you create a Linux container where you'll work to learn the basics (thus skip the Introduction part of the video tutorial) docker run -d -t --name ubuntu ubuntu #start a basic Linux container docker exec -it ubuntu bash -c 'apt update && apt install g++ -y' #install the C++ compiler, you will need it! Note: See how you can attach vscode to the container to start developing!","title":"C++"},{"location":"tutorial/#cmake","text":"As you will learn from the video tutorial, you need to manually call the C++ compiler, which is the g++ executable, to turn your human-readable code into binary instructions for the machine. Imagine that your project includes many programs to compile. Do you need to call g++ manually for each file? No, of course, you can use CMake! CMake is a cross-platform (Windows/Linux) tool that makes the generation of executables easier. No wonder that is the tool you will need to use to compile the code you'll write to form a new ROS package. For this purpose, you will learn to use this tool in the ROS tutorials, but even here a minimum preliminary understanding of CMake is required. Completing Step 1 and Step 2 of the official tutorial will be enough!","title":"CMake"},{"location":"tutorial_intermediate/","text":"Tutorials (Intermediate) Before going on, let's go back to the docker run hello-world command. What docker run does is to create a container from the hello-world image. You can create as many containers (completely unrelated) as you want from the same image. Note: We suggest you the vscode docker extension to explore all containers and images on your system. You should also have noticed that docker run will download (or rather pull ) the hello-world image from the web (actually from some known repositories, like apt does) if it was not found locally. Try one by one the commands below. docker rmi hello-world #will erase the local image docker pull hello-world #only downloads the latest version of the image. docker run hello-world #start a container with random fancy name docker run --name pippo hello-world #start a container named pippo docker run --name pluto --rm hello-world #start a container named pippo, automatically remove it on stop (--rm) docker container ls #list all running container Dockerfiles In you went through the ROS tutorial , you probably installed some apt packages to proceed, or maybe you manually downloaded some code from git . The point is, if you loose the container for some reason, will you also loose all the installations you made, all the files you downloaded, within the container? The answer is \"yes\". That's the reason why we use a Dockerfile to keep track of most of the installations. Let's consider the learn the C++ basics example. docker run -d -t --name ubuntu ubuntu #start a basic Linux container: -d sends it in background (detach), -t keeps it running docker exec -it ubuntu bash -c 'g++ --version' #gives error, g++ is not installed yet docker exec -it ubuntu bash -c 'apt update && apt install g++ -y' #install the C++ compiler, you will need it! We can keep track of the g++ installation with a Dockerfile (create it from vscode) FROM ubuntu RUN apt update && apt install g++ -y How it works? Save it and use it to build a specialized Docker Image docker build -f /path/to/your/Dockerfile -t my-image so that if you start a container from the newly created image you will find g++ already there docker run -d -t --name my-ubuntu my-image docker exec -it ubuntu bash -c 'g++ --version' #no error, prints the compiler version Docker Compose Docker Containers can be composed together to form a modular architecture. For instance consider a practical case on our teleoperation platform. You are developing a super cool ROS2 package which turns a target 6D pose for the robot into target joint angles for the robotic arms. The vendor, Universal Robots in our case, provides us the ROS2 driver package, which will listen for your joint angle commands (via a ros2 topic ) and will take care to send them to the robots. The point is, are you sure that you want to test you're new algorithm directly on real (very expensive) robots? Wouldn't you like to see first how the math that you implemented behaves? Maybe you'll find some bug, so it's a good practice that you test your codes on simulated robots first. Remember, simulated arms are free! Here's where modularity helps you! Let's say that your ROS2 package is installed along with the UR driver into a dedicated container, let's call it drivers . You only need to start a second container which runs the simulator to start debugging. Whenever you're ready to test with real robots, just stop the simulator container and connect your robots to drivers container. Moreover, imagine that a colleague of yours has a container, let's call it devices , which turns mocap data into target 6D poses for the robots. You can just compose devices and drivers together and the teleoperation system is ready to go!","title":"Tutorials (Intermediate)"},{"location":"tutorial_intermediate/#tutorials-intermediate","text":"Before going on, let's go back to the docker run hello-world command. What docker run does is to create a container from the hello-world image. You can create as many containers (completely unrelated) as you want from the same image. Note: We suggest you the vscode docker extension to explore all containers and images on your system. You should also have noticed that docker run will download (or rather pull ) the hello-world image from the web (actually from some known repositories, like apt does) if it was not found locally. Try one by one the commands below. docker rmi hello-world #will erase the local image docker pull hello-world #only downloads the latest version of the image. docker run hello-world #start a container with random fancy name docker run --name pippo hello-world #start a container named pippo docker run --name pluto --rm hello-world #start a container named pippo, automatically remove it on stop (--rm) docker container ls #list all running container","title":"Tutorials (Intermediate)"},{"location":"tutorial_intermediate/#dockerfiles","text":"In you went through the ROS tutorial , you probably installed some apt packages to proceed, or maybe you manually downloaded some code from git . The point is, if you loose the container for some reason, will you also loose all the installations you made, all the files you downloaded, within the container? The answer is \"yes\". That's the reason why we use a Dockerfile to keep track of most of the installations. Let's consider the learn the C++ basics example. docker run -d -t --name ubuntu ubuntu #start a basic Linux container: -d sends it in background (detach), -t keeps it running docker exec -it ubuntu bash -c 'g++ --version' #gives error, g++ is not installed yet docker exec -it ubuntu bash -c 'apt update && apt install g++ -y' #install the C++ compiler, you will need it! We can keep track of the g++ installation with a Dockerfile (create it from vscode) FROM ubuntu RUN apt update && apt install g++ -y How it works? Save it and use it to build a specialized Docker Image docker build -f /path/to/your/Dockerfile -t my-image so that if you start a container from the newly created image you will find g++ already there docker run -d -t --name my-ubuntu my-image docker exec -it ubuntu bash -c 'g++ --version' #no error, prints the compiler version","title":"Dockerfiles"},{"location":"tutorial_intermediate/#docker-compose","text":"Docker Containers can be composed together to form a modular architecture. For instance consider a practical case on our teleoperation platform. You are developing a super cool ROS2 package which turns a target 6D pose for the robot into target joint angles for the robotic arms. The vendor, Universal Robots in our case, provides us the ROS2 driver package, which will listen for your joint angle commands (via a ros2 topic ) and will take care to send them to the robots. The point is, are you sure that you want to test you're new algorithm directly on real (very expensive) robots? Wouldn't you like to see first how the math that you implemented behaves? Maybe you'll find some bug, so it's a good practice that you test your codes on simulated robots first. Remember, simulated arms are free! Here's where modularity helps you! Let's say that your ROS2 package is installed along with the UR driver into a dedicated container, let's call it drivers . You only need to start a second container which runs the simulator to start debugging. Whenever you're ready to test with real robots, just stop the simulator container and connect your robots to drivers container. Moreover, imagine that a colleague of yours has a container, let's call it devices , which turns mocap data into target 6D poses for the robots. You can just compose devices and drivers together and the teleoperation system is ready to go!","title":"Docker Compose"},{"location":"wsl/","text":"WSL The Windows Subsystem for Linux if powerfull, but tricky. If you're using Docker Desktop, the trickiest part is running high-consuming rendering application (within a container) using your favourite GPU. Otherwise, even simple programs like Rviz! will overload your CPU. After some investigations we found a solution. ROS1 FROM osrf/ros:noetic-desktop ARG DEBIAN_FRONTEND=noninteractive SHELL [\"/bin/bash\", \"-c\"] RUN apt update -y && apt upgrade -y && apt -y install \\ libxext-dev \\ libx11-dev \\ libglvnd-dev \\ libglx-dev \\ libgl1-mesa-dev \\ libgl1-mesa-glx \\ libgl1-mesa-dri \\ libegl1-mesa-dev \\ libgles2-mesa-dev \\ freeglut3-dev \\ mesa-utils \\ mesa-utils-extra RUN apt install -y software-properties-common && add-apt-repository ppa:kisak/turtle -y && apt update && apt upgrade -y ENV MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA ENV LD_LIBRARY_PATH=/usr/lib/wsl/lib ENV LIBVA_DRIVER_NAME=d3d12 and test with docker run --device /dev/dxg --device /dev/dri/card0 --device /dev/dri/renderD128 --env DISPLAY=$DISPLAY --env WAYLAND_DISPLAY=$WAYLAND_DISPLAY --env PULSE_SERVER=$PULSE_SERVER --volume /tmp/.X11-unix:/tmp/.X11-unix --volume /mnt/wslg:/mnt/wslg --volume /usr/lib/wsl:/usr/lib/wsl -dit --gpus=all --name noetic wsl2-opengl:noetic ROS2 Way simpler docker run --device /dev/dxg --device /dev/dri/card0 --device /dev/dri/renderD128 --env DISPLAY=$DISPLAY --env WAYLAND_DISPLAY=$WAYLAND_DISPLAY --env PULSE_SERVER=$PULSE_SERVER --env MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA --env LD_LIBRARY_PATH=/usr/lib/wsl/lib --env LIBVA_DRIVER_NAME=d3d12 --volume /tmp/.X11-unix:/tmp/.X11-unix --volume /mnt/wslg:/mnt/wslg --volume /usr/lib/wsl:/usr/lib/wsl -dit --gpus=all --name jazzy osrf/ros:jazzy-desktop Clock If you encounter any issue with ROS time, which keeps resetting, look at this repo . Of course you need to install the service within the WSL distribution that serves as Docker Desktop backend. Networking If you need to contact some windows applications or some physical device via TCP, you'll need to open the WSL Settings app. Set the network mode to Mirrored and enable the host address loopback option. Then restart WSL.","title":"WSL"},{"location":"wsl/#wsl","text":"The Windows Subsystem for Linux if powerfull, but tricky. If you're using Docker Desktop, the trickiest part is running high-consuming rendering application (within a container) using your favourite GPU. Otherwise, even simple programs like Rviz! will overload your CPU. After some investigations we found a solution.","title":"WSL"},{"location":"wsl/#ros1","text":"FROM osrf/ros:noetic-desktop ARG DEBIAN_FRONTEND=noninteractive SHELL [\"/bin/bash\", \"-c\"] RUN apt update -y && apt upgrade -y && apt -y install \\ libxext-dev \\ libx11-dev \\ libglvnd-dev \\ libglx-dev \\ libgl1-mesa-dev \\ libgl1-mesa-glx \\ libgl1-mesa-dri \\ libegl1-mesa-dev \\ libgles2-mesa-dev \\ freeglut3-dev \\ mesa-utils \\ mesa-utils-extra RUN apt install -y software-properties-common && add-apt-repository ppa:kisak/turtle -y && apt update && apt upgrade -y ENV MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA ENV LD_LIBRARY_PATH=/usr/lib/wsl/lib ENV LIBVA_DRIVER_NAME=d3d12 and test with docker run --device /dev/dxg --device /dev/dri/card0 --device /dev/dri/renderD128 --env DISPLAY=$DISPLAY --env WAYLAND_DISPLAY=$WAYLAND_DISPLAY --env PULSE_SERVER=$PULSE_SERVER --volume /tmp/.X11-unix:/tmp/.X11-unix --volume /mnt/wslg:/mnt/wslg --volume /usr/lib/wsl:/usr/lib/wsl -dit --gpus=all --name noetic wsl2-opengl:noetic","title":"ROS1"},{"location":"wsl/#ros2","text":"Way simpler docker run --device /dev/dxg --device /dev/dri/card0 --device /dev/dri/renderD128 --env DISPLAY=$DISPLAY --env WAYLAND_DISPLAY=$WAYLAND_DISPLAY --env PULSE_SERVER=$PULSE_SERVER --env MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA --env LD_LIBRARY_PATH=/usr/lib/wsl/lib --env LIBVA_DRIVER_NAME=d3d12 --volume /tmp/.X11-unix:/tmp/.X11-unix --volume /mnt/wslg:/mnt/wslg --volume /usr/lib/wsl:/usr/lib/wsl -dit --gpus=all --name jazzy osrf/ros:jazzy-desktop","title":"ROS2"},{"location":"wsl/#clock","text":"If you encounter any issue with ROS time, which keeps resetting, look at this repo . Of course you need to install the service within the WSL distribution that serves as Docker Desktop backend.","title":"Clock"},{"location":"wsl/#networking","text":"If you need to contact some windows applications or some physical device via TCP, you'll need to open the WSL Settings app. Set the network mode to Mirrored and enable the host address loopback option. Then restart WSL.","title":"Networking"}]}